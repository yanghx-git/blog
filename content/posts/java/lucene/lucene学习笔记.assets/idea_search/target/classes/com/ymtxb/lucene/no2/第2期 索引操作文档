
第1部分 索引增加文档 删除文档 修改文档
--------------------------------------------------------------------------（开始）
->向索引增加文档
添加索引的方法有两个：
1. indexWriter.addDocument(Document doc)
    使用默认分析器添加文档，该分析器在创建indexWriter对象时指定，用于语汇单元分析操作。

2. indexWriter.addDocument(Document doc, Analyzer analyzer)
    使用指定的分析器添加文档和语汇单元分析操作。 (注意：新版本不支持该API接口)
（分析Demo1添加文档）
关闭writer或者显示调用writer.commit()都会将缓冲区内的索引数据落盘。

->删除文档
大部分情况大家更关注如何将文档加入到Lucene索引中，但是也有部分例外，比如下面这种情况。
例：某资讯平台只保留半月内的最新资讯，需要将过期的资讯全部删除。
还有一些程序需要删除包含某一项的全部文档，都可以通过IndexWriter提供的方法来从索引中删除文档。
1. indexWriter.deleteDocuments(Term term) 负责删除包含项的所有文档。
2. indexWriter.deleteDocuments(Term[] terms) 负责删除包含项数组任一元素的所有文档。
3. indexWriter.deleteDocuments(Query query) 负责删除匹配查询语句的所有文档。
4. indexWriter.deleteDocuments(Query[] querys) 负责删除匹配查询语句数组任一元素的所有文档。
5. indexWriter.deleteAll() 负责删除索引中全部文档。

如果需要通过Term删除单个文档，需要确认文档在索引时索引了指定Field字段，还需要确认该Field域字段值
都是唯一的，才可以将该文档单独定位出来并删除。概念类似于数据库中的主键，需要注意该域需要设置为“不分析”的域，
保证分析器不会将它分解成语汇单元。
例：writer.deleteDocuments(new Term("id", documentId))
注意：调用该方法时一定要谨慎，如果不小心创建了错误的Term对象（被分词的域，可能关联很多文档），那么Lucene
将删除索引中与该Term命中的全部文档。
删除操作并不会立即执行，而是会存放在缓冲区内，与加入文档类似，最后Lucene会通过周期性刷新文档目录来执行该操作。
也可以通过writer.commit() 或者 writer.close() 来立即生效。即使删除操作已经完成，物理磁盘上的文件也不会立即
删除，Lucene只是将被删除的文档标记为“删除”，待索引段合并时会进行真正的物理删除。
（分析Demo1删除文档）

->更新索引中的文档
更新已索引的文档这个需求很常见，比如你的搜索系统依赖的源数据进行的更新，那么相对应的索引就必须进行更新，
否则搜索系统的搜索准确度会降低。某些情况下，仅仅是文档的某个域需要更新，如产品名称发生了变化，但是正文
未变化，非常遗憾，尽管该需求很普遍，但是Lucene做不到。Lucene只能删除整个过期文档，然后再向索引中添加
新文档。这要求新文档必须包含旧文档的所有域，包括内容未发生改变的域。
IndexWriter 提供了1个API来更新索引中的文档
    indexWriter.updateDocument(Term term, Document newDoc)
    第一步 删除term匹配的文档，第二步 使用writer再添加文档。
例：writer.updateDocument(new Term("id", documentId), newDocument)
注意：由于updateDocument方法在后台会调用deleteDocuments方法，一定要确定Term标识的唯一性。
（分析DemoUpdateDocument）
--------------------------------------------------------------------------（结束）

第2部分 域类型
--------------------------------------------------------------------------（开始）
老版本中Lucene并没有提供许多内置类型的Field，而是统一使用原生Field类型。
代码例如：注意，这些代码都不能在新版本Lucene使用了，有更好的方式。
// 存储，不分词 且 不存储NORMS（加权基准）
doc.add(new Field("id", ids[i], Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
// 存储，不分词
doc.add(new Field("author", authors[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
// 存储，进行分词
doc.add(new Field("title", titles[i], Field.Store.YES, Field.Index.ANALYZED));


|--------------|--------------|--------------|--------------|--------------|------------------------------------------|
    Filed类          数据类型        是否分词        是否索引        是否存储                        说明
|--------------|--------------|--------------|--------------|--------------|------------------------------------------|
|  StringField |     字符串    |      No      |      Yes     |     Y/N      |  用来构建一个字符串Field，但是不会进行分词，会将整个串当做一个“语汇单元”存储到索引中，比如“订单号”“身份证号”，是否存储原值由参数 Store 决定
|  LongPoint   |    Long型    |      Yes     |       Yes    |      Y/N     |   用来构建一个Long型Field，进行分词和索引，比如“价格”。是否存储在文档中由Store决定
|  StoredField |    多种类型   |      No      |       No     |      Yes     |   不分析，不索引，但是Field会存储在文档中
|  TextField   |    字符串或流  |      Yes     |      Yes     |      Y/N     |  如果值参数是Reader，Lucene猜测值比较大，会采用Unstored策略
|--------------|--------------|--------------|--------------|--------------|------------------------------------------|

->Reader、TokenStream、byte[]域值
TextField(String name, Reader reader, Store store)
该构造方法使用Reader而非String来表示域值，在这种情况下域值是不能被存储的（域存储选项被硬编码为Store.NO），并且该域将用于分析和索引。
如果在内存中保存String代价比较高或者不太方便时，如存储的域值比较大，请使用这个初始化方法。

TextField(String name, TokenStream tokenStream)
TokenStream 用来表示一组已分析过的语汇单元，Analyze分析器内部也是使用的TokenStream来表示分析过的语汇单元列表，
可以这样理解，不使用IndexWriter进行分析了，提前一步将“内容”分析过了，分析过的内容由 TokenStream 引用。

StringField(String name, ByteRef byteRef, Store store)
  BytesRef(byte[] bytes, int offset, int length)
这种域不会被索引，只会做存储。注意，Store在底层会强设YES。一般很少用..


--------------------------------------------------------------------------（结束）

第3部分 加权操作
--------------------------------------------------------------------------（开始）
-> 域加权
所谓域加"权"，就是根据需求的不同，对不同的关键值或者不同的关键索引分配不同的权值，因为查询的时候Lucene的评分机制和权值的高低是成正比的，
这样权值高的内容更容易被用户搜索出来，而且排在前面。
field.setBoost(float boost)


-> 文档加权（4.x之后Lucene取消了文档加权，想实现文档加权就让文档的所有域一起加权）
假设有这样的需求，你为公司设计Email索引程序，该程序可能要求在进行索引结果排序时，公司员工的Email能排在其他Email前面位置。
如何实现呢？
其实很简单，默认情况下，所有文档都没有加权值（或者说 加权值 都是 1.0）。通过修改文档的加权因子，你就能指示Lucene在计算相关性
时或多或少的考虑到该文档针对其他文档的重要程度。
document.setBoost(float boost)
(文档加权demo)

--------------------------------------------------------------------------（结束）

第4部分 索引特殊字段
--------------------------------------------------------------------------（开始）
-> 索引数字
程序处理数字有两个截然不同的场景。
场景一：数字内嵌在将要索引的文本中，需要将数字视为单独的语汇单元来处理。这样就可以在随后的搜索程序中实现查找数字筛选文档。
例如 “Be sure to include From 1099 in your tax return.” 你希望能搜索到数字“1099”，就像搜索 “tax” 一样。
实现该功能非常简单，只需要提供一个不丢弃数字的分析器即可，比如 “WhitespaceAnalyzer” “StandardAnalyzer”，它们都会将数字
当做单独的语汇单元来处理。

场景二：文档中特定域只包含数字，而你希望将他们作为数字域值来索引，并能在搜索和排序中对它们进行精确匹配。
例如 产品列表中每一款产品都有价格字段，你希望在搜索功能支持按照价格区间来搜索产品。

在老版本Lucene（2.9之前），并不能很好的支持数字索引，需要特别处理数字，采用补0的手段。
问题：不进行补0有什么问题？
比如有 123456，123，222 被当做普通语汇单元来处理，在进行范围搜索“TermRangeQuery” [123,222]，
会将 123456,123,222 全部查询出来，很明显“123456”不应该被定位出来..
因为Lucene索引语汇单元的顺序是按照字典序来的（上层是一个跳跃表结构），最终索引的顺序为 123,123456,222
采用补0方案之后这些数字转换为了：000123456，000000123,000000222
这样的话最终索引的顺序就成为了：000000123,000000222,000123456
该方案虽然也能完成范围查询的需求，但是很明显“性能很烂”。如果我的搜索范围比较大[000000123,000000222]，
内部其实是将范围查询 转换为 多个 Term 查询，然后再做的合并。
再一个缺点，你很难预测到底补多少个0才能满足日后需求。

在新版本Lucene，已经可以很好的支持数字索引了，IntPoint LongPoint FloatPoint DoublePoint 这些Field类型来支持不同精度的数值域。
通过将数字转化为一些字符串来完成的索引，比如 num1 拆解成 a ab abc   num2 拆解成 a ab abd。
|------------|------------|
|     num1   |     num2   |
|------------|------------|
| a ab abc   | a ab abd   |
|------------|------------|
通过搜索ab 可以把带ab 前缀的num1,num2 都找出来。在范围查找的时候，查找范围内的数值的相同前缀可以达到一次查找返回多个doc 的目的，从而减少查找次数。
至于如何将数值转化为对应的 字符串组合，这点大家可以去查阅下资料或者源码。底层其实是通过 位 来计算出来的，咱们基础课先不纠结这么多了，有个了解就成了。

-> 索引日期和时间
该问题其实和索引数字是一个问题。你可以将日期和时间转换为等价的数值。
doc.add(new LongPoint("timestamp", new Date().getTime()));

--------------------------------------------------------------------------（结束）

第5部分 域截取
--------------------------------------------------------------------------（开始）
添加索引文档时，很有可能某个文档的正文特别长，导致Lucene分析过程比较费时。此时你可能只想对文档
中的部分内容进行索引。比如说 只对文档的前200个单词进行索引。
为了支持该需求，IndexWriter允许你对域进行截取后再索引它们。实例化IndexWriter之后，在向其提供
MaxFieldLength实例去控制截取数量。MaxFieldLength有两个默认的实例（MaxFieldLength.UNLIMITED / MaxFieldLength.LIMITED），
MaxFieldLength.UNLIMITED：不采取截取策略，该策略。
MaxFieldLength.LIMITED：只截取域中前1000个项。
当然你也可以自己去实例化MaxFieldLength去控制截取长度。需要注意的一点是，域截取意味着程序会忽略一部分文档文本，会使得
这些文本无法通过Lucene来搜索，从而让用户感到困惑，所以使用域截取功能时一定要谨慎，或者只有在真正有必要时再使用它。
--------------------------------------------------------------------------（结束）

第6部分 段合并
--------------------------------------------------------------------------（开始）
-> 分段存储
早期全文检索工具为全部文档维护一个很大的索引结构，并将其存入磁盘。如果有更新操作，那么就全量重建
该索引结构。该方式很明显不再适合当今时代，你必须要清楚重建索引的成本是最高的！至于为什么不能修改原
索引结构，你可以想象一下“在黑板上已经写好的一篇课文，现在要向其中间插一句话”，有几种方案？
1. 空白的角落写一句话，然后告诉同学们
2. 这句话后面的字全部擦掉，然后插进去，再将后面的那些被擦掉的字写到“新插”后面。
Lucene采用的方案类似方案2，选择的重建索引。方案1看似虽好，但是随着时间的流逝熵会越来越大，不利于搜索内核程序。

现在Lucene引入了“索引分段”的技术，即将一个索引文件拆分为多个子索引文件。每个段都是一个独立的可被搜索的数据集，
并且段具有不变性，一旦写入磁盘，就不能再发生改变。

分段策略下，索引的写入过程如下：
1. 新增。当有新的数据需要创建索引时，由于段的不变性，所以选择新建一个段来存储新增的数据。
（注意：其实是先写到内存缓冲区，当缓冲区满【16mb】 或者 自动刷盘（1秒） 或者 commit()/close() 都会创建新的段文件）
2. 删除。当需要删除数据时，由于数据所在的段只可读，不可写，所以Lucene在索引文件下新增了一个.del的文件，用来专门存储被删除的数据id。
当查询时，被删除的数据还是可以被查到的，只是在进行文档链表合并时，才把已经删除的数据过滤掉。被删除的数据在进行段合并时才会真正被移除。
3. 更新。更新的操作其实就是删除和新增的组合，先在.del文件中记录旧数据，再在新段中添加一条更新后的数据。

为了提升性能，Lucene并不会每新增一个文档就生成一个新段，而是采用延迟写的策略，数据先写入内存缓冲区，然后根据策略再批量写入硬盘。
若有一个段被写到硬盘，就会生成一个提交点，提交点就是一个用来记录所有提交后的段信息的文件。
一个段一旦拥有了提交点，就说明这个段只有读的权限，失去了写的权限；
相反，当段在内存中时，就只有写数据的权限，而不具备读数据的权限，所以也就不能被检索。

-> 段合并策略（索引优化）
虽然分段存储策略解决了增量索引问题，但是由于每次新增数据时都可能会产生新的段，经过一段时间的积累，会导致索引中存在大量的段。
当索引中段的数量太多时，不仅会严重消耗服务器的资源（打开索引文件会占用文件数），还会影响检索的性能。
因为分段之后的检索过程其实是 “查询所有段中满足查询条件的数据，然后对每个段里查询的结果集进行合并”。
所以为了控制索引里段的数量，我们必须定期进行段合并操作。
肯定有同学会问什么是段合并？答：将多个小段的索引数据读取到内存中，在内存中合并成一个索引段，然后再写入磁盘，再将旧段文件全部删除。
在执行了flush、commit等方法后，Lucene会基于段的合并策略对索引目录中的段集合进行合并操作。
Lucene在IndexWriter类中也提供了额外的方法允许用户可以主动去执行段的合并操作。
API:
indexWriter.forceMerge(int maxSegments)
将索引合并到指定maxSegments个段，合并后段最多可达到maxSegments个，此参数传1，则将全部段合并为一个大段。

关于合并策略相关的参数，课程暂不讲解了，基础课程的目的是告诉你Lucene的大致工作逻辑，想要进一步学习的同学，可以去看源码。
--------------------------------------------------------------------------（结束）
