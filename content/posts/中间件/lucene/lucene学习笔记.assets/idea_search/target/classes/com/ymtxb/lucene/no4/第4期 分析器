第1部分 分析器
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~（开始）
-> 分析是什么？
概念：分析（Analysis），在Lucene中指的是将域（Field）文本转换为最基本的索引表示单元————项（Term）的过程。
分析器对分析操作进行了封装，它通过执行若干个操作，将文本转换为语汇单元，这些操作可能包括：提取单词、去除标点符号、字母转换小写、
去除常用词、将单词还原为词干形式或基本形式...。这个处理过程被称为“语汇单元化过程（tokenization）”，从文本流中提取的文本块
称为语汇单元。语汇单元与它的域名结合后，就形成了项（Term）。

-> 使用分析器
分析操作出现在任何需要将文本转换成项的时刻，主要有两个时间点：
1. 建立索引期间
2. 使用QueryParser解析查询期间

demo（AnalysisTest）
在建立索引时，通过分析过程提取的语汇单元就是被索引的项。只有被索引的项才能被搜索到！

-> 常用分析器简介
1. WhitespaceAnalyzer：该分析器通过空格来分割文本信息，并不会对生成的语汇单元进行其他的规范化处理。
2. SimpleAnalyzer：该分析器通过非字母字符来分割文本信息，然后将语汇单元统一为小写形式。注意，该分析器会去掉数字类型的字符，但会保留其他字符。
3. StopAnalyzer：与SimpleAnalyzer类似，区别是，本分析器会去除常用词。（比如：a the is ...）
4. StandardAnalyzer：这是Lucene最复杂的核心分析器。它包含大量的逻辑操作来识别某些种类的语汇单元，比如公司名称、E-mail地址以及主机名称等等..
它也会将语汇单元转换成小写形式，并去除停用词和标点符号。

-> 索引过程中的分析
 (图-1)

-> QueryParser使用分析器
QueryParser能够很好的为搜索用户提供形式自由的查询。为了完成这个任务，QueryParser使用分析器将文本信息分割成各个项以用于搜索。
示例代码：
QueryParser parser = new QueryParser("contents", analyzer);
Query query = parser.parse(expression);

分析器会接受表达式中连续的独立的文本片段，而不是整体接受整个表达式。
例如下面这个查询语句：
"stephen curry" +warrior +champion
QueryParser 会分3次调用分析器，首先是处理"stephen curry"，然后是 warrior，最后是 champion。
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~（结束）

第2部分 剖析分析器
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~（开始）
-> 语汇单元的组成
语汇单元流（tokenStream）是分析过程中所产生的基本输出。在索引时，Lucene使用特定的分析器来处理需要被语汇单元化的域，
而每个语汇单元相关的重要属性随即被编入索引中。

（图-2）
以“the quick brown fox”的分析为例。该文本中每个语汇单元都表示一个独立的单词。一个语汇单元携带了一个文本值（即单词本身）和其他一些元数据：
偏移量、语汇单元类型（默认：word）、以及位置增量。语汇单元可以选择性包含一些由程序定义的标志位和任意字节数的有效负载，这样程序就能根据具体需要
来处理这些语汇单元。
起点的偏移量是指语汇单元文本的起始字符在原始文本中的位置，而终点偏移量则表示语汇单元文本终止字符的下一个位置。
语汇单元的类型是用String对象表示的，默认值是“word”，如果需要的话，可以在语汇单元过滤过程中控制和利用该类型属性。
文本被语汇单元化之后，相对于前一个语汇单元的位置信息以位置增量值保存。大多数内置的语汇单元化模块都将位置增量的默认值设置为1，
表示所有语汇单元都是连续的，在位置上是一个接一个的。每个语汇单元还带有多个选择标志；一个标志即32bit数据集（以int型数值保存），
Lucene内置的分析器并不使用这些标志，但你自己设计的搜索程序是可以使用它们的，每个语汇单元都能以byte[]数组形式将负载信息记录在索引中。

-> 语汇单元转换为项
当文本在索引过程中进行过分析后，每个语汇单元都作为一个项被传递给索引。

语汇单元中都有哪些信息会存储到索引中呢？
1. 语汇单元值本身
2. 位置增量
3. 偏移量
4. 有效载荷（payload）
语汇单元的的类型和标志位都将被抛弃————它们只在分析过程中使用。

-> 位置增量
位置增量使得当前语汇单元和前一个语汇单元在位置上关联起来。一般来说位置增量为1，表示每个单词存在于域中唯一且连续的位置上。
位置增量因子会影响短语查询和跨度查询，因为这些查询需要知道域中各个项之间的距离。
如果位置增量大于1，则表示本语汇单元与前一个语汇单元中间有空隙（删除的词）。
如果位置增量为0，表示该语汇单元与前一个语汇单元在相同的位置上。同义词分析器可以通过0增量来插入同义词语汇单元。
这个做法使得Lucene在进行短语查询时，输入任意一个同义词都能匹配到同一结果。

-> 语汇单元流（TokenStream）
TokenStream 是一个能在被调用后产生语汇单元序列的类，TokenStream类有两个不同的实现：
1. Tokenizer：通过java.io.Reader对象读取字符并创建语汇单元。
2. TokenFilter：持有一个TokenStream，TokenFilter负责处理输入的语汇单元，通过新增、删除、修改属性的方式来影响结果。

分析器从它的tokenStream()方法返回tokenStream对象后，该tokenStream就开始利用tokenizer对象创建初始的语汇单元序列，
然后再链接任意数量的tokenFilter对象来修改这些语汇单元。被称为分析器链（analyzer chain）。

Reader --> Tokenizer --> TokenFilter --> TokenFilter --> TokenFilter --> tokens
（分析器链以一个Tokenizer对象开始，通过Reader对象读取字符并产生初始语汇单元，然后用任意数量链接的TokenFilter对象修改这些语汇单元）


---------------Lucene核心API提供的分析器构建模块-------------------------
1. TokenStream          抽象Tokenizer基类
2. Tokenizer            输入参数为Reader对象的TokenStream对象
3. CharTokenizer        基于字符的Tokenizer父类，包含抽象方法isTokenChar()。当isTokenChar()为true
                        时输出连续的语汇单元块。
4. WhitespaceTokenizer  CharTokenizer的子类（实现类），当字符是非空格时，isTokenChar() 返回true，空格时返回false
5. KeywordTokenizer     将输入的整个字符串转换为一个语汇单元
6. LetterTokenizer      CharTokenizer的子类（实现类），当字符是字母时，isTokenChar() 返回true，否则返回false
7. LowerCaseTokenizer   LetterTokenizer的子类，在其基础上，添加了将字符转换为小写的功能
8. StandardTokenizer    复杂而基于语法的语汇单元生成器，用于输出高级别类型的语汇单元，如 E-mail 地址...
9. TokenFilter          输入参数为另一个TokenStream子类的TokenStream
10. LowerCaseFilter     TokenFilter的子类（实现类），用于将语汇单元转换为小写
11. StopFilter          FilteringTokenFilter的子类（实现类），过滤掉指定停用词集合内的语汇单元
12. PorterStemFilter    利用Porter词干提取算法将语汇单元还原为其词干形式。例如：单词 country 和 countries 还原为词干 countri
13. CachingTokenFilter  将上层TokenStream中的语汇单元一次性全部获取出来，并缓存在该层Filter
14. LengthFilter        FilteringTokenFilter的子类（实现类），支持特定长度的语汇单元限制，非标准的将过滤掉
---------------Lucene核心API提供的分析器构建模块-------------------------

-> 语汇单元的组成
（demo：AnalysisTest.testFullDetails()）

TokenStream继承类AttributeSource，TokenStream内置了一些属性，当你通过addAttribute(Class)时，内部首先查询是否有该Class的属性对象，
如果有则返回，如果没有内部会查询该Class属性对应的Impl类型，然后创建Impl实例，存放到map内。TokenStream向AttributeSource提供一个factory，
该factory内置了一些Token会用到的Impl类型。

--------------------------Lucene内置的语汇单元属性---------------------------
1. CharTermAttribute                语汇单元对应的文本
2. PositionIncrementAttribute       位置增量（默认值1）
3. OffsetAttribute                  起始字符和终止字符的偏移量
4. TypeAttribute                    语汇单元类型（默认：word）
5. FlagsAttribute                   自定义标志位
6. PayloadAttribute                 每个语汇单元的byte[]类型的有效负载
--------------------------Lucene内置的语汇单元属性---------------------------
该表中的属性值是双向的，你可以通过它来获取当前Token的属性值，也可以通过它来修改当前Token的属性值。

TokenStream提供了captureState()和restoreState()方法，captureState()方法会返回一个包含当前Token的所有
属性信息的State克隆对象，restoreState(state)可以将当前Token属性恢复至state状态。

-> 起始位置和终止位置有什么作用？
起始位置和结束位置记录的是指定“Token”在初始字符中的位置偏移量，它们对于每个语汇单元来说并不透明，你可以在这里设置任意的整数。
其实，在Lucene写索引期间会将位置信息记录到词向量文件中，方便搜索时高亮关键词。

-> 语汇单元类型的作用？
索引并不会记录语汇单元的类型，语汇单元类型存在的意义是告知后续TokenFilter当前Token的类型，方便后续TokenFilter根据
类型做不同的逻辑..

-> 语汇单元过滤器：过滤顺序的重要性
对于一些TokenFilter子类来说，在分析过程中对事件的处理顺序非常重要。移除停用词的处理就是一个很好的例子。
StopFilter类在停用词集合中区分大小写地检查每一个语汇单元，这个步骤就依赖输入小写形式的语汇单元。
（demo：AnalysisTest.testStopAnalyzer2()）
（demo：AnalysisTest.testStopAnalyzerFlawed()）
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~（结束）

第3部分 内置分析器
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~（开始）

----------------------------------Lucene几个主要的内置分析器（begin）--------------------------------------
1. WhitespaceAnalyzer                   根据空格拆分语汇单元
2. SimpleAnalyzer                       根据非字母字符拆分文本，并将其转换为小写形式
3. StopAnalyzer                         根据非字母字符拆分文本，然后小写，再移除停用词
4. KeywordAnalyzer                      将整个文本作为一个单一语汇单元处理
5. StandardAnalyzer                     基于复杂的语法来生成语汇单元，该语法能识别E-mail地址、首字母缩写词、汉语/日语/韩语字符、
                                        字母数字等。还能完成小写转换和移除停用词
----------------------------------Lucene几个主要的内置分析器（end）----------------------------------------


-> StopAnalyzer
StopAnalyzer 分析器除了完成基本的单词拆分和小写化功能之外，还负责移除一些称之为停用词（stop words）的特殊单词。
停用词是指较为通用的词（如“the”），它对于搜索来说无意义，几乎每个文档都会包含这样的词。
StopAnalyzer内置了一个英文停用词集合ENGLISH_STOP_WORDS_SET，该集合数据如下：
a、an、and、are、as、at、be、but、by、for、if、in、into、is、it、no、not、of、on、or、such、that、the、that、their、
then、there、these、they、this、to、was、will、with
StopAnalyzer有一个可重载的构造方法，允许你通过传入自己的停用词集合。

-> 应该采用哪种核心分析器？
答案比较震惊：大多数应用程序都不使用任意一种内置分析器，而是选择创建自己的分析器链。
一般来讲，应用程序都有自己的特殊需求，如自定义的停用词列表、为程序特定的语汇单元（领域内的词汇）等等。
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~（结束）

第4部分 案例同义词分析器
看代码吧...